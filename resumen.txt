# Resumen Técnico del Proyecto Chatbot Normativa UFRO

Este documento explica el funcionamiento del código principal del proyecto, sus componentes, el flujo de datos y posibles puntos de fallo. Está orientado a que una inteligencia artificial pueda entender el sistema y responder preguntas técnicas o ayudar en tareas de depuración y mejora.

---

## 1. Estructura General

El proyecto implementa un chatbot RAG (Retrieval-Augmented Generation) para responder preguntas sobre normativa universitaria. Utiliza Flask como backend web, modelos LLM (ChatGPT, DeepSeek), embeddings y FAISS para recuperación de contexto.

### Componentes principales:

- **app.py**: Backend Flask, expone endpoints web y coordina el flujo RAG.
- **providers/**: Adaptadores para distintos proveedores LLM (ChatGPT, DeepSeek).
- **rag/**: Módulos para recuperación de contexto, embeddings, prompts.
- **data/**: Documentos fuente, índice FAISS, metadatos.
- **templates/**: Interfaz web HTML.
- **Dockerfile**: Contenedor para despliegue.
- **.env**: Claves API y configuración.

---

## 2. Flujo de Ejecución (app.py)

### Inicialización

- Carga variables de entorno (.env) para claves API.
- Instancia Flask y habilita CORS.
- Llama a `initialize_rag()`:
  - Crea el objeto Retriever (carga embeddings y FAISS).
  - Instancia el proveedor LLM (por defecto ChatGPT).
  - Guarda todo en el diccionario global `RAG_SYSTEM`.

### Endpoints

- **/**  
  Renderiza la interfaz web (`index.html`).

- **/query [POST]**  
  Recibe una pregunta del usuario, ejecuta el pipeline RAG:
  1. Recupera contexto relevante con Retriever (FAISS).
  2. Construye el prompt con los fragmentos recuperados.
  3. Llama al proveedor LLM (ChatGPT/DeepSeek) para generar la respuesta.
  4. Verifica si la respuesta contiene evidencia/citas (heurística regex).
  5. Si no hay evidencia, responde con abstención y fuentes recuperadas.
  6. Devuelve la respuesta, fuentes y fragmentos al frontend.

### CLI Local

- Si se ejecuta directamente (`__main__`), lanza Flask en modo debug.

---

## 3. Proveedores LLM (providers/)

- **base.py**: Define la interfaz abstracta `Provider`.
- **chatgpt.py**: Implementa `ChatGPTProvider` usando la API de OpenAI.
  - Lee la clave de entorno.
  - Método `chat()` envía mensajes y recibe respuesta.
  - Maneja errores y devuelve mensaje de error si falla la API.

---

## 4. Recuperación de Contexto (rag/)

- **Retriever**: Busca los fragmentos más relevantes en los documentos usando embeddings y FAISS.
- **build_messages**: Construye el prompt para el LLM con el contexto y la pregunta.

---

## 5. Evidencia y Abstención

- Función `has_evidence()` usa expresiones regulares para detectar si la respuesta contiene citas, URLs, referencias, etc.
- Si no hay evidencia, el sistema responde con un mensaje de abstención y sugiere consultar a la unidad académica.

---

## 6. Docker y Despliegue

- **Dockerfile**: Prepara el entorno Python, instala dependencias, copia datos y código, expone el puerto 8000 y lanza Gunicorn.
- **.dockerignore**: Excluye archivos temporales y entornos virtuales.

---

## 7. Posibles Puntos de Falla

- **Inicialización**: Si faltan claves API o archivos de embeddings/FAISS, el sistema no se inicializa.
- **Importaciones**: Si la estructura de carpetas es incorrecta, fallan los imports.
- **Proveedor LLM**: Si la API de OpenAI/DeepSeek no responde o la clave es inválida, falla la generación.
- **Recuperación**: Si FAISS no encuentra contexto relevante, el sistema responde con abstención.
- **Evidencia**: La heurística puede fallar si el LLM no cita explícitamente, aunque la respuesta sea correcta.
- **Docker**: Si faltan dependencias del sistema (ej. FAISS), la imagen puede fallar al construir.

---

## 8. Preguntas que una IA puede responder

- ¿Cómo se inicializa el sistema y qué dependencias requiere?
- ¿Cómo se recupera el contexto relevante para una pregunta?
- ¿Cómo se construye el prompt para el LLM?
- ¿Cómo se detecta si una respuesta tiene evidencia documental?
- ¿Qué ocurre si el sistema no encuentra contexto relevante?
- ¿Cómo se manejan los errores de la API LLM?
- ¿Cómo se puede cambiar el proveedor LLM o el modelo de embeddings?
- ¿Cómo se agregan nuevos documentos fuente?
- ¿Cómo se despliega el sistema en Docker?

---

## 9. Mejoras posibles

- Mejorar la heurística de evidencia para respuestas más precisas.
- Permitir selección dinámica de proveedor LLM desde el frontend.
- Implementar logging estructurado y métricas de uso.
- Añadir autenticación para el endpoint /query.
- Mejorar la gestión de errores y mensajes al usuario.

---

Este resumen permite a una IA entender el sistema, identificar puntos críticos y responder preguntas técnicas sobre el funcionamiento y la depuración del chatbot.